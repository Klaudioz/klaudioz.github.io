<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>My chia farming rig</title>
      <link href="/2021/05/13/My-chia-farming-rig/"/>
      <url>/2021/05/13/My-chia-farming-rig/</url>
      
        <content type="html"><![CDATA[<ul><li>Chia</li><li>Cryptocurrency</li><li>Hardware</li></ul><p>This is a short but a very interesting project. I’ve worked intensively during 3 weeks on it and it’s giving me a side income very fast, due the value of the coin is about $1.000 right now even when it was listed for first time on a exchange just few days ago. It’s a very new coin and it looks promising. Maybe something like BTC 7-8 years ago.</p><p>This project is getting a lot of traction maybe because its creator: Bram Cohen known as Bittorrent protocol creator (P2P) and another good reason is that is using a new technology to generate the coins, called <a href="https://www.chia.net/faq/">Proof of Space</a>, which is using hard drive space instead brute force calculations so it’s using a lot of less energy compared to the rest of the coins like BTC or ETH. You can check this <a href="https://chiapower.org/">website</a> for more details.</p><p>You can check the source code of chia-blockchain in its <a href="https://github.com/Chia-Network/chia-blockchain">GitHub</a> repository. The coin has a very active community on: <a href="https://keybase.io/team/chia_network.public">Keybase</a>. Also you can interact in side communities like: <a href="https://www.reddit.com/r/chia/">Reddit</a> or <a href="https://chiaforum.com/">ChiaForum</a>.</p><p>Basically to “farm” chia, the process is divided in two steps which can be done with the same computer or in separated one in a network. The first process is called plotting and it’s the creation of the plot. A plot is a about 100GB file (if you use k=32) which is generated with a powerful computer. This process needs a computer with a fast CPU (8-cores should be good) and a super fast SSD called NVME which should be the most expensive part of this. This disk should have  a very high TBW (Total Bytes Written) to be able to work longer. When you generate a 100GB plot, this disk is generating about 2-3TB of space written so this process can kill a disk with a low TBW in few days.</p><p>For this rig, the RAM should be 32GB, and the speed of that is not important. If you want to see a reference plotter machine you can visit this <a href="https://chiadecentral.com/budget-plotting-build/">website</a>. My first plotter machine is the same showed here, next three machine I bought them from a ETH miner who take off the graphic card. I added 16GB RAM additional, bought the NVME disk and I got a used cheap graphic card just to start the computer.<br>You can get a very powerful a expensive machine getting between 3-9 TB/Day or 30-90 plots/day (aproximately) but at the end if you are looking for the inflection point or the lowest $/TiB/day, you can get it with a 500 USD machine like <a href="https://chianetwork.blogspot.com/2021/04/build-install-guide-budget-plotting.html">this</a> but if you have 10 of these machines for example, you should always consider the electricity bill.</p><p>The second stage is the farming, where can be done in a separated a very low resource machine, like a Raspberry Pi or Rock Pi and it just need to keep the hard disks, full of plots, connected trough USB using a 16-Port USB Hub like <a href="https://www.amazon.com/dp/B07KHRLSTT/ref=cm_sw_r_oth_api_glt_fabc_G348P4VA2HR4YE5SX62N?_encoding=UTF8&psc=1">this</a> and just wait to get a coin. While more plots you have, more chances to win but it depends of the size of the entire network, which is growing exponentially so in my case, my odds to win are close to 2 XCH (You receive 2 XCH every time you win) each 8 days. You always can simulate this on <a href="https://chiacalculator.com/">link</a>.</p><p>So, these are picture of my rig. One picture is from the day and another one at the night. At night is easier to check if one USB disks is turned off.</p><p><img src="chia-day.jpg"><br><img src="chia-night.jpg"></p><p>In this system I have a lot of 8TB USB disks which is not optimal on space, heat and electricity but I did it because the prices of SATA disk went up a lot, so it was the best solution I found to have a big farm.</p><p>Now you can see the details of the rig I’ve created:</p><ul><li><p><strong>5 plotters machines</strong>: 4 of them are very similar with 8 core CPU and 32 RAM and the one on the left is a 500 USD experiment which is giving me about 12 plots daily (<a href="https://chianetwork.blogspot.com/2021/04/build-install-guide-budget-plotting.html">https://chianetwork.blogspot.com/2021/04/build-install-guide-budget-plotting.html</a>). The 3 of the right I got them from an ETH miner who bought them to take off the credit card. So I was very lucky to find this guy with a complimentary hobby. I got them for about 700 USD each and I needed to buy the NVME disks, 16GB more of RAM, and very cheap graphics cards just to start the machines, because I manage them using SSH.</p></li><li><p><strong>70x 8TB Seagate from Costco</strong>. I got them for 120 USD each and I was lurking several Costco places and doing experiments to buy them fast. There was a online limit of 3 units so I bought them all from the stores. Fortunately in Utah I didn’t have a lot of competition and there were plenty of disks in any Costco place I visited. The Costco url is not working anymore but as a reference, this is the <a href="https://www.amazon.com/Seagate-Backup-Desktop-Recovery-Services/dp/B07KFG2ZYN">disk</a>.</p></li><li><p><strong>SATA Disks</strong>: I got 1x18TB, 1x16TB, 4x14TB Seagate Exos disks. I got them previously the prices went up. For example I bought the 18TB one at 399 USD. <a href="https://www.amazon.com/dp/B08K98VFXT">18 TB Link</a>. <a href="www.diskprices.com">diskprices.com</a> is a good place to check HDD prices. As a reference, my 8TB disks were 15 USD/TB.</p></li><li><p><strong>NVME disks</strong>: 1x 4GB, 2x 2GB, 2x 1GB and 2x512GB. I did some experiments trying to figure out what is the best combination. Until now the best result I got it from the <a href="https://www.amazon.com/dp/B08FCY3BM2">1 TB</a> disks using RAID 0.</p></li><li><p><strong>Some NVME PCI adapters</strong>. Some motherboards I have only have one of these ports. These double ones work pretty good <a href="https://www.amazon.com/dp/B08CBTC348">Amazon Link</a>.</p></li><li><p><strong>Two Mr ironstone bar table 47’’ tables and 4 Long heavy-duty power strip 16 from Amazon</strong>. I put them together because I was extremely lucky that the size of them is almost the same, so it was very easy to put the strip under the tables. <a href="https://www.amazon.com/dp/B07P3H4P1D">Table link</a> and <a href="https://www.amazon.com/dp/B08G1CMQKZ">Strip link</a>.</p></li><li><p><strong>Two UPS Cyberpower 1500 VA</strong> from BestBuy. The UPS I had collapsed when I connected the 3er outlet and I needed to buy these very fast, so I got them from a near Best Buy store as the extension wires. <a href="https://www.bestbuy.com/site/cyberpower-1500va-battery-back-up-system-black/3938817.p?skuId=3938817">Best Buy link</a>.</p></li><li><p><strong>8x USB 3.0 extension cables</strong>: The second row of 32 disks which is at the floor can’t reach the computer at the top so I needed these extension cables. <a href="https://www.bestbuy.com/site/insignia-6-usb-3-0-extension-cable-a-male-to-a-female-black/">Link</a>. You can get cheapest ones in Amazon. I needed to buy them because I need them now because my trip.</p></li><li><p><strong>TP-Link Gigabit Switch with CAT7 wires</strong>: Nothing to add here. Just plug &amp; play. It’s very cheap. <a href="https://www.amazon.com/dp/B00A121WN6">Amazon Link Switch</a> and <a href="https://www.amazon.com/dp/B07R525KRL">Amazon Link wires</a>.</p></li></ul><p>My initial plan was to have separated the plotters from the farmers. I have 2 rock pi 4A machines for that, but the 16x USB I had for that failed, and because of my trip, I had to move fast and I finished with this. In my return, if I’d buy more disks I’ll try again probably with this separation.</p><p>About the total size it is about: 8<em>70 + 18 + 16 + 4</em>14 = 650TB but not the 100% of the space is usable so it’d be around 600TB total and I’m able to generate about 5500 plots.</p><p>I’m using right now one of these rock pi 4 to access to the plotters, using it as a bastion host with fail2ban to limit hacking attempts. I have a gigabit router for communication and I’m using CAT6 wires but I couldn’t get any improvement compared to a normal cable.</p><p>I’m getting about <strong>70-80 plots</strong> daily with this setup and I invested around <strong>16K</strong> in all of that. I hope to optimize it a little bit and getting 90-100 plots daily but having only 600TB (I’ve done 60TB previous the trip), if it’s going too fast it’ll fill the disks previous to my return so I hope to come back around the same day when all the disks are full. Also chia is releasing a new version soon to allow pools, so it’s possible to have to re-create the plots so if my disks are done while I’m out, I’ll consider this alternative but always the idea is to have the biggest amount of plots available.</p><p>This is my first experience doing something like that with “real hardware”. I’ve built my own computers all my life but nothing compared to this. Also, the total price is the sum of all my previous rigs together.</p><p>Finally working as a DevOps Engineer I’ve good experience managing Linux as a sysadmin, creating scripts and automation. But even that it was very good to have living practice dealing with formatting, mounting disks, learning about NVME and checking temps in an intense environment.</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Top 50 new lightweight kubernetes tools</title>
      <link href="/2020/09/21/Top-50-new-kubernetes-tools/"/>
      <url>/2020/09/21/Top-50-new-kubernetes-tools/</url>
      
        <content type="html"><![CDATA[<p>In this post I filtered out a big collection about new useful tools to work with Kubernetes. I used the following criteria to select them:</p><ul><li>Open source projects hosted in Github.</li><li>More than 50 stars but less than 10000 to exclude very popular tools like: <a href="https://github.com/helm/helm">helm</a>, <a href="https://github.com/kubernetes-sigs/kustomize">kustomize</a>, <a href="https://github.com/grafana/loki">loki</a>, <a href="https://github.com/lensapp/lens">lens</a>, <a href="https://github.com/kubernetes-sigs/kind">kind</a>, etc.</li><li>Last commit not old than 3 months.</li></ul><p>The motivation to create this list was feeling overwhelmed with the amount of new Kubernetes tools you can see every day so making a sorted list managing this knowledge can be done easily.<br>So, this is the final list without any particular sorting except the name of the categories:</p><h2 id="Alerting"><a href="#Alerting" class="headerlink" title="Alerting"></a>Alerting</h2><ul><li><a href="https://github.com/flant/k8s-image-availability-exporter">k8s-image-availability-exporter</a>: Alert if an image used in Kubernetes cannot be pulled from container registry.</li></ul><h2 id="Backup"><a href="#Backup" class="headerlink" title="Backup"></a>Backup</h2><ul><li><a href="https://github.com/miracle2k/k8s-snapshots">k8s-snapshots</a>: Automatic Volume Snapshots on Kubernetes.</li><li><a href="https://github.com/stashed/stash">stash</a>: Backup Kubernetes Stateful Applications.</li></ul><h2 id="Best-practices"><a href="#Best-practices" class="headerlink" title="Best practices"></a>Best practices</h2><ul><li><a href="https://github.com/FairwindsOps/polaris">polaris</a>: Validation of best practices in your Kubernetes clusters.</li></ul><h2 id="Chaos-engineering"><a href="#Chaos-engineering" class="headerlink" title="Chaos engineering"></a>Chaos engineering</h2><ul><li><a href="https://github.com/litmuschaos/litmus">litmus</a>: It helps Kubernetes SREs and developers practice chaos engineering in a Kubernetes native way.</li></ul><h2 id="CLI-tools"><a href="#CLI-tools" class="headerlink" title="CLI tools"></a>CLI tools</h2><ul><li><a href="https://github.com/ahmetb/kubectx">kubectx</a>: Faster way to switch between clusters and namespaces in kubectl</li><li><a href="https://github.com/jonmosco/kube-ps1">kube-ps1</a>: Kubernetes prompt info for bash and zsh</li><li><a href="https://github.com/andreazorzetto/yh">yh</a>: YAML syntax highlighter to bring colours where only jq could.</li><li><a href="https://github.com/helm/chart-testing">chart-testing</a>: CLI tool for linting and testing Helm charts.</li><li><a href="https://github.com/FairwindsOps/pluto">pluto</a>: A cli tool to help discover deprecated apiVersions in Kubernetes.</li><li><a href="https://github.com/cyberark/kubeletctl">kubeletctl</a>: It’s a CLI tool that implement kubelet’s API.</li><li><a href="https://github.com/narendranathreddythota/podtnl">podtnl</a>: CLI tool that makes your pod available online without exposing a Kubernetes service.</li><li><a href="https://github.com/sbstp/kubie">kubie</a>: A more powerful alternative to kubectx and kubens.</li><li><a href="https://github.com/d-kuro/kubectl-fuzzy">kubectl-fuzzy</a>: This tool uses fzf(1)-like fuzzy-finder to do partial or fuzzy search of Kubernetes Resources.</li></ul><h2 id="Cluster-management"><a href="#Cluster-management" class="headerlink" title="Cluster management"></a>Cluster management</h2><ul><li><a href="https://github.com/futurewei-cloud/arktos">arktos</a>: It’s an open source cluster management system designed for large scale clouds addressing key challenges of large scale clouds, including system scalability, resource efficiency, multitenancy, etc.</li><li><a href="https://github.com/kubecost/cluster-turndown">cluster-turndown</a>: Automated turndown of Kubernetes clusters on specific schedules.</li><li><a href="https://github.com/kubermatic/kubermatic">kubermatic</a>: Kubermatic Kubernetes Platform - the Central Kubernetes Management Platform for any infrastructure.</li><li><a href="https://github.com/kubermatic/kubecarrier">KubeCarrier</a>: It’s a system for managing applications and services across multiple Kubernetes Clusters; providing a framework to centralize the management of services and provide these services with external users in a self service catalog.</li><li><a href="https://github.com/weaveworks/wksctl">wksctl</a>: Allows simple creation of a Kubernetes cluster given a set of IP addresses and an SSH key. It can be run in a standalone environment but is best used via a GitOps approach in which cluster and machine descriptions are stored in Git.</li></ul><h2 id="Container-registry"><a href="#Container-registry" class="headerlink" title="Container registry"></a>Container registry</h2><ul><li><a href="https://github.com/plexsystems/sinker">sinker</a>: A tool to sync images from one container registry to another.</li></ul><h2 id="Costs"><a href="#Costs" class="headerlink" title="Costs"></a>Costs</h2><ul><li><a href="https://github.com/kubecost/cost-model">cost-model</a>: Cross-cloud cost allocation models for workloads running on Kubernetes.</li></ul><h2 id="Deployment"><a href="#Deployment" class="headerlink" title="Deployment"></a>Deployment</h2><ul><li><a href="https://github.com/practo/k8s-worker-pod-autoscaler">k8s-worker-pod-autoscaler</a>: It scales the replicas in a deployment based on observed queue length.</li><li><a href="https://github.com/dpeckett/pangolin">pangolin</a>: It’s an enhanced Horizontal Pod Autoscaler for Kubernetes. Pangolin scales deployments based on their Prometheus metrics, using a variety of highly configurable control strategies.</li><li><a href="https://github.com/Clivern/Beetle">beetle</a>: Kubernetes multi-cluster deployment automation service.</li></ul><h2 id="Documentation"><a href="#Documentation" class="headerlink" title="Documentation"></a>Documentation</h2><ul><li><a href="https://github.com/norwoodj/helm-docs">helm-docs</a>: A tool for automatically generating markdown documentation for helm charts.</li><li><a href="https://github.com/kubepack/chart-doc-gen">chart-doc-gen</a>: Helm Chart documentation generator.</li></ul><h2 id="GitOps"><a href="#GitOps" class="headerlink" title="GitOps"></a>GitOps</h2><ul><li><a href="https://github.com/fluxcd/flux">flux</a>: The GitOps Kubernetes operator.</li></ul><h2 id="Kubernetes-as-a-service"><a href="#Kubernetes-as-a-service" class="headerlink" title="Kubernetes as a service"></a>Kubernetes as a service</h2><ul><li><a href="https://github.com/oneinfra/oneinfra">oneinfra</a>: Tool designed to orchestrate fleet of Kubernetes clusters. You can control cloud as well as on-prem clusters.</li></ul><h2 id="Kubernetes-API"><a href="#Kubernetes-API" class="headerlink" title="Kubernetes API"></a>Kubernetes API</h2><ul><li><a href="https://github.com/swade1987/deprek8ion">Deprek8ion</a>: Rego policies to monitor Kubernetes APIs deprecations.</li></ul><h2 id="Logging"><a href="#Logging" class="headerlink" title="Logging"></a>Logging</h2><ul><li><a href="https://github.com/vmware/kube-fluentd-operator">kube-fluentd-operator</a>: Auto-configuration of Fluentd daemon-set based on Kubernetes metadata.</li></ul><h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><ul><li><a href="https://github.com/infracloudio/botkube">botkube</a>: It helps you monitor your Kubernetes cluster, debug critical deployments &amp; gives recommendations for standard practices.</li><li><a href="https://github.com/prometheus-operator/kube-prometheus">kube-prometheus</a>: Use Prometheus to monitor Kubernetes and applications running on Kubernetes</li></ul><h2 id="Networking"><a href="#Networking" class="headerlink" title="Networking"></a>Networking</h2><ul><li><a href="https://github.com/omrikiei/ktunnel">ktunnel</a>: A cli that exposes your local resources to kubernetes. It’s convenient replacement for ngrok.</li><li><a href="https://github.com/soluble-ai/kubetap">kubetap</a>: Kubectl plugin to interactively proxy Kubernetes Services with ease.</li></ul><h2 id="Operator"><a href="#Operator" class="headerlink" title="Operator"></a>Operator</h2><ul><li><a href="https://github.com/clastix/capsule">capsule</a>: It’s a Kubernetes multi-tenant Operator. It can workaround the flat structure of namespaces in Kubernetes by introducing an abstraction called Tenant. Within each tenant, users are free to create their namespaces and share all the assigned resources.</li><li><a href="https://github.com/operator-framework/operator-lifecycle-manager">operator-lifecycle-manager</a>: A management framework for extending Kubernetes with Operators.</li><li><a href="https://github.com/alexellis/registry-creds">registry-creds</a>: Automate Kubernetes registry credentials, to extend Docker Hub limits.</li></ul><h2 id="Secrets"><a href="#Secrets" class="headerlink" title="Secrets"></a>Secrets</h2><ul><li><a href="https://github.com/kubernetes-sigs/secrets-store-csi-driver">Secrets Store CSI</a>: Secrets Store CSI driver for Kubernetes secrets - Integrates secrets stores with Kubernetes via a CSI volume.</li></ul><h2 id="Security"><a href="#Security" class="headerlink" title="Security"></a>Security</h2><ul><li><a href="https://github.com/appvia/krane">krane</a>: It’s a Kubernetes RBAC static analysis tool. It identifies potential security risks in K8s RBAC design and makes suggestions on how to mitigate them. Krane dashboard presents current RBAC security posture and lets you navigate through its definition.</li><li><a href="https://github.com/cyberark/KubiScan">kubiscan</a>: A tool to scan Kubernetes cluster for risky permissions (RBAC).</li><li><a href="https://github.com/deepfence/ThreatMapper">ThreatMapper</a>: It identifies vulnerabilities in running containers, images, hosts and repositories.</li><li><a href="https://github.com/banzaicloud/dast-operator">dast-operator</a>: Dynamic Application and API Security Testing.</li><li><a href="https://github.com/darkbitio/mkit">mkit</a>: MKIT is a Managed Kubernetes Inspection Tool that validates several common security-related configuration settings of managed Kubernetes cluster objects and the workloads/resources running inside the cluster.</li><li><a href="https://github.com/cruise-automation/k-rail">k-rail</a>: Kubernetes security tool for policy enforcement.</li></ul><h2 id="Sign-on"><a href="#Sign-on" class="headerlink" title="Sign-on"></a>Sign-on</h2><ul><li><a href="https://github.com/authelia/authelia">authelia</a>: The Single Sign-On Multi-Factor portal for web apps.</li></ul><h2 id="Storage"><a href="#Storage" class="headerlink" title="Storage"></a>Storage</h2><ul><li><a href="https://github.com/configurator/kubefs">kubefs</a>: Mount kubernetes metadata storage as a filesystem.</li><li><a href="https://github.com/opstree/dynamic-pv-scaler">dynamic-pv-scaler</a>: It’s a golang based Kubernetes application which has been created to overcome the scaling issue of Persistent Volume in Kubernetes. This can scale the Persistent Volume on the basis of threshold which you have set.</li></ul><h2 id="Testing"><a href="#Testing" class="headerlink" title="Testing"></a>Testing</h2><ul><li><a href="https://github.com/Stono/kconmon">kconmon</a>: It’s a Kubernetes node connectivity tool that performs frequent tests (tcp, udp and dns), and exposes Prometheus metrics.</li></ul><h2 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h2><ul><li><a href="https://github.com/weibeld/k1s">k1s</a>: It’s the world’s simplest Kubernetes dashboard with only 30 lines of bash code.</li><li><a href="https://github.com/oslabs-beta/Allok8">allok8</a>: It’s a dynamic, conventional, and uncomplicated web-based UI Kubernetes visualization tool.</li></ul><p>I hope you can find something new and interesting in this list to help you to manage in a more efficient way anything related with Kubernetes. If you have any comments or suggestions please leave a comment.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Tools </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Is 100% availability a myth?</title>
      <link href="/2020/08/31/Is-100-availability-a-myth/"/>
      <url>/2020/08/31/Is-100-availability-a-myth/</url>
      
        <content type="html"><![CDATA[<p>Quick answer: <strong>Yes</strong>, high availability is not about being a hundred percent available. It’s about being close to that. Actually, the word is <strong>high</strong> availability. It’s not total availability or full availability.</p><p>A hundred percent uptime with no failures and with no humans involved is an utopia, and not just that. It’s also a wrong goal because the nature of http prevents you from always being a 100% HA (High availability). The chances that not a single request will ever be lost are impossible for a system with an important number of activity.</p><p>It’s not just a technical or money challenge, it’s a human issue, and as humans we will fail. It involves everything and anything can fail any moment. We can plan and implement very reliable architectures, but get a 100% HA is an infinite resources issue. I think we can understand easily this phenomenon with an exponential function graph like:</p><p><img src="2020-08-31-14-40-31.png"></p><p>I’ve put some invented figures which are giving a good idea about what you’ll see in the graph. Basically, a 100% HA will have an infinite cost. It’s the nature of exponential numbers. Even having an infinite budget for implementing this extremely complex architecture, it’s needed also a 100% HA of your internet, electric sources and another invisible system for a common user like <a href="https://blog.cloudflare.com/cloudflare-outage-on-july-17-2020/">Cloudflare</a>, cloud availability zones or regions, etc.</p><p>Seeing the graph, my recommendation is to find the <strong>inflection point</strong> between the costs and availability. We can calculate it with the <strong>second derivative</strong> of the graph.</p><p>Another important question is: Maybe right now is impossible to get 100% HA but what about in the future?. It’s very to predict anything, specially in this industry where the knowledge is changing so fast. The only constant is the change and complexity going up so, in my opinion, it’ll be very difficult for me to see this utopia as a reality, during my lifetime 😬</p><p>This post is based in the last episode of <a href="https://www.devopsparadox.com/episodes/high-availability-does-not-mean-100-availability-70/">DevOps Paradox</a> podcast which I can highly recommend you.</p>]]></content>
      
      
      
        <tags>
            
            <tag> HA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Container Security Fundamentals - Summary</title>
      <link href="/2020/08/17/Container-Security-Fundamentals-Summary/"/>
      <url>/2020/08/17/Container-Security-Fundamentals-Summary/</url>
      
        <content type="html"><![CDATA[<p>Today, Kubecon EU 2020 has started as an online event because of the pandemic so I’m starting a series of posts summarizing the most important topics covered. Hopefully, you found this helpful.</p><p>I’m starting with the presentation called <a href="https://kccnceu20.sched.com/event/ZetO/help-my-cluster-is-on-the-internet-container-security-fundamentals-samuel-davidson-google"> Help! My Cluster Is On The Internet: Container Security Fundamentals</a> by <a href="https://www.linkedin.com/in/samuelbdavidson/">Samuel Davidson</a> from Google. All the credits for him.</p><p>There are 4 areas where we can improve our security related to Kubernetes. These areas are Containers, Pods, Cluster and User.</p><ul><li><p>Containers</p><ul><li>#1 - Assume you will be owned:<ul><li>Always new vulnerabilities will be found. That’s inevitable because of the complexity of every software. So it’s better to be prepared all the time to mitigate this risk working with best security practices.</li></ul></li><li>#2 - Use a distroless base image:<ul><li>Distroless images are bare-bones versions of common base images. They have the bare-minimum needed to execute a binary. So if an attacker gains access, it’s not possible to do a lot of damage.</li><li>When you are interested to reduce size, Alpine should be your choice. For security, the choice should be distroless. Both are very reduced size and implementing security by the concept of <strong>Less attack surface</strong> by avoiding unnecessary stuff on the packaged docker images but distroless has no a package manager at all.</li><li>You can check Google’s GitHub repository of distroless docker images in the following <a href="https://github.com/GoogleContainerTools/distroless">link</a></li></ul></li><li>#3 - Containers should be easy to rebuild and redeploy:<ul><li>It’d be an automated process by a CI/CD pipeline. Having full automation should be the goal. It can work even as a <strong>disaster recovery strategy</strong> because you can rebuild your entire infrastructure from zero quickly.</li></ul></li><li>#4 - Trust your containers with signatures:<ul><li>Your CI/CD platform should sign the containers it builds for production and the image repository is trusted through TLS.</li><li>Think about the possibility that one attacker pushed an infected version of your code in the container registry.</li><li>So, a container with a not valid signature is not accepted in the cluster. It’s not implemented natively by Kubernetes, but it can be done with a <strong>policy engine</strong> like <a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper</a></li></ul></li></ul></li><li><p>Pods</p><ul><li>#1 - Don’t use hostPath:<ul><li>Avoid using the Node’s filesystem. It can be edited for other pods, apps, etc.</li><li>There are better alternatives like <em>PersistentVolumeClaim</em>.</li></ul></li><li>#2 - Don’t use <em>hostNetwork</em>:<ul><li>Binds the pod to the Node’s network. Allows localhost communication with K8s infrastructure components running on the node.</li></ul></li><li>#3 - Be conscious of your pod’s Service Account:<ul><li><strong>All pods</strong> have a service account by default even if you don’t set one in the <em>podSpec</em>.</li><li>Then, the credentials for your pods service account are automatically mounted within its filesystem and available to the container which can be owned by an attacker !!</li></ul></li><li>#4 Avoid privileged containers in your pods:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">containers:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">image:</span> <span class="string">klaudioz/hello-world</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">hello-world</span></span><br><span class="line">    <span class="attr">securityContext:</span></span><br><span class="line">      <span class="attr">privileged:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure><ul><li>This pod is owned by <em>root</em>, so if an attacker owns the pod, it can be very dangerous.</li></ul></li></ul></li><li><p>Cluster</p><ul><li>#1 - Keep your cluster up to date:<ul><li>Bugs and vulnerabilities are fixed all the time, also Kubernetes provides a secure incremental way to update the nodes of the cluster one by one.</li></ul></li><li>#2 - Isolate your cluster from the Internet:<ul><li>Your nodes should not be on the internet with a publicly addressable IP. Use a <em>LoadBalancer</em> for that.</li></ul></li><li>#3 - For your secrets use secrets:<ul><li>Great for access keys, passwords, tokens, etc. Stored in memory, never saved to a node.</li><li>The security of Secrets comes from <em>RBAC</em> and using many namespaces as security boundaries.</li></ul></li><li>#4 Don’t use basic auth:<ul><li>It is a quick convenient way to set up usernames and passwords with your API Server using the flag:    <code>--basic-auth-file=...</code></li><li><strong>NEVER</strong> use it in a production cluster.</li></ul></li></ul></li><li><p>User</p><ul><li>#1 - All your devs and robots (CICD, automation) should have a unique identity:<ul><li>Every person or service account should have a unique identity to interact with the cluster.</li></ul></li><li>#2 - Use <em>RBAC</em> and groups:<ul><li>Implement the <a href="https://en.wikipedia.org/wiki/Principle_of_least_privilege">Principle of Least Privilege</a> in your cluster with RBAC’s fine-grained controls in an easy way.</li></ul></li><li>#3 - Using <em>RBAC</em>, bind policies against groups not individuals:<ul><li>Use <em>Roles</em> and <em>Bindings</em> within the cluster:</li><li><img src="2020-08-17-11-07-08.png"></li></ul></li><li>#4 - Use a policy agent to protect your cluster:<ul><li>Typically a Kubernetes <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">AdmissionController</a> which selectively allows/denies Kubernetes resource requests based on rules (or policies)</li><li>It can be used <a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper</a> for this job. It’s very easy to configure and make it works.</li></ul></li></ul></li></ul><p>The author provided a summary document from the talk <a href="https://docs.google.com/document/d/1OefvSpURuOHdNNiwbRXmhPiRUFsYnllgMPDaVqXRkh4/edit#heading=h.lroslfbjn64r">here</a> if you have any question related to the talk.</p>]]></content>
      
      
      
        <tags>
            
            <tag> Kubernetes </tag>
            
            <tag> Kubecon </tag>
            
            <tag> Security </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
